{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39023bed-3a7d-44eb-9987-bc920e9db24b",
   "metadata": {},
   "source": [
    "# Source: \n",
    "https://open.data.gov.sa/ar/datasets/view/0fd9a088-8bd9-4d8a-8d69-63eac103238d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42944a9-4933-4446-95e5-3d82c34ec70d",
   "metadata": {},
   "source": [
    "# DataSet Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2800db-ae99-4ec1-a4f7-ea2f577415f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Transactions sale for real estate CSV.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a34a1f3-dc30-4153-95cc-05ad38abebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "المنطقة (14 unique):\n",
      "['منطقة مكة المكرمه' 'منطقة الرياض' 'منطقة نجران' 'منطقة عسير'\n",
      " 'منطقة الباحة' 'منطقة الشرقية' 'منطقة المدينة المنوره' 'منطقة جازان'\n",
      " 'منطقة تبوك' 'منطقة الجوف']\n",
      "--------------------------------------------------\n",
      "المدينة (155 unique):\n",
      "['الطائف' 'الرياض' 'شروره' 'حوطة بني تميم' 'الدلم' 'الافلاج' 'بيشه'\n",
      " 'رجال المع' 'عفيف' 'بلجرشي']\n",
      "--------------------------------------------------\n",
      "المدينة / الحي (4323 unique):\n",
      "['الطائف/ الجودية ' 'الطائف/ الشهداء الجنوبية ' 'الرياض/الزهراء'\n",
      " 'شروره/ الوديعة ' 'شروره/ النزهة ' 'الرياض/الدريهمية' 'الرياض/بدر'\n",
      " 'الطائف/ وادي جليل ' 'حوطة بني تميم/ الورود ' 'حوطة بني تميم/ نجد ']\n",
      "--------------------------------------------------\n",
      "الرقم المرجعي للصفقة (65729 unique):\n",
      "[27006520. 27051687. 27038443. 27048618. 27042546. 27045125. 27047908.\n",
      " 27049909. 27050165. 27044359.]\n",
      "--------------------------------------------------\n",
      "تاريخ الصفقة ميلادي (91 unique):\n",
      "['2025/01/01' '2025/01/02' '2025/01/03' '2025/01/04' '2025/01/05'\n",
      " '2025/01/06' '2025/01/07' '2025/01/08' '2025/01/09' '2025/01/10']\n",
      "--------------------------------------------------\n",
      "تاريخ الصفقة هجري (91 unique):\n",
      "['1446/07/01' '1446/07/02' '1446/07/03' '1446/07/04' '1446/07/05'\n",
      " '1446/07/06' '1446/07/07' '1446/07/08' '1446/07/09' '1446/07/10']\n",
      "--------------------------------------------------\n",
      "تصنيف العقار (4 unique):\n",
      "['سكني' 'تجاري' 'زراعي' nan]\n",
      "--------------------------------------------------\n",
      "عدد العقارات (28 unique):\n",
      "[ 1.  3.  2. 11.  8.  6.  4.  7. 12.  5.]\n",
      "--------------------------------------------------\n",
      "السعر (12825 unique):\n",
      "['275,000' '523,335' '1,000,000' '1,800,000' '40,000' '150,000' '687,664'\n",
      " '1,300,000' '450,000' '500,000']\n",
      "--------------------------------------------------\n",
      "المساحة (20224 unique):\n",
      "['760.50' '360.00' '354.42' '348.75' '625.00' '900.00' '156.63' '600.00'\n",
      " '146.40' '935.00']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    unique_vals = df[column].unique()\n",
    "    print(f\"{column} ({len(unique_vals)} unique):\")\n",
    "    print(unique_vals[:10])  # show only first 10 values to keep it readable\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f027ed7-dc98-4bf1-a86f-0ac6b8540c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Arabic column names to English\n",
    "df.rename(columns={\n",
    "    'المنطقة': 'Region',\n",
    "    'المدينة': 'City',\n",
    "    'المدينة / الحي': 'City/Neighborhood',\n",
    "    'الرقم المرجعي للصفقة': 'Transaction ID',\n",
    "    'تاريخ الصفقة ميلادي': 'Transaction Date (Gregorian)',\n",
    "    'تاريخ الصفقة هجري': 'Transaction Date (Hijri)',\n",
    "    'تصنيف العقار': 'Property Classification',\n",
    "    'نوع العقار': 'Property Type',\n",
    "    'عدد العقارات': 'Property Count',\n",
    "    'السعر': 'Price',\n",
    "    'المساحة': 'Area'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cc4bd5d-ab02-4857-8c1a-0cfa345577e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping = {\n",
    "    'منطقة مكة المكرمة': 'Makkah',\n",
    "    'منطقة الرياض': 'Riyadh',\n",
    "    'منطقة نجران': 'Najran',\n",
    "    'منطقة عسير': 'Asir',\n",
    "    'منطقة الباحة': 'Al Bahah',\n",
    "    'منطقة الشرقية': 'Eastern Province',\n",
    "    'منطقة المدينة المنوره': 'Medina',\n",
    "    'منطقة جازان': 'Jazan',\n",
    "    'منطقة تبوك': 'Tabuk',\n",
    "    'منطقة الجوف': 'Al Jawf',\n",
    "    'منطقة القصيم': 'Al Qassim',\n",
    "    'منطقة حائل': 'Hail',\n",
    "    'منطقة الحدود الشمالية': 'Northern Borders'\n",
    "}\n",
    "\n",
    "city_translation = {\n",
    "    'الطائف': 'Taif',\n",
    "    'الرياض': 'Riyadh',\n",
    "    'شروره': 'Sharurah',\n",
    "    'حوطة بني تميم': 'Houtat Bani Tamim',\n",
    "    'الدلم': 'Ad-Dilam',\n",
    "    'الافلاج': 'Al-Aflaj',\n",
    "    'بيشه': 'Bisha',\n",
    "    'رجال المع': 'Rijal Almaa',\n",
    "    'عفيف': 'Afif',\n",
    "    'بلجرشي': 'Baljurashi',\n",
    "    'الدمام': 'Dammam',\n",
    "    'مرات': 'Marat',\n",
    "    'الليث': 'Al Lith',\n",
    "    'المدينة المنورة': 'Medina',\n",
    "    'الشقيق': 'Ash-Shuqaiq',\n",
    "    'طريب': 'Turaif',\n",
    "    'جده': 'Jeddah',\n",
    "    'النعيريه': 'An-Nairyah',\n",
    "    'ضرماء': 'Dhurma',\n",
    "    'مكة المكرمة': 'Makkah',\n",
    "    'المهد': 'Al Mahd',\n",
    "    'تبوك': 'Tabuk',\n",
    "    'سكاكا': 'Sakaka',\n",
    "    'رماح': 'Rumah',\n",
    "    'الخبر': 'Al Khobar',\n",
    "    'السليل': 'As Sulayyil',\n",
    "    'بريده': 'Buraidah',\n",
    "    'تثليت': 'Tathlith',\n",
    "    'الدوادمي': 'Ad-Dawadmi',\n",
    "    'يدمه': 'Yadamah',\n",
    "    'الارطاويه': 'Al Artawiyah',\n",
    "    'العقيق': 'Al-Aqiq',\n",
    "    'الحناكية': 'Al Hanakiya',\n",
    "    'رنيه': 'Ranyah',\n",
    "    'طبرجل': 'Tabarjal',\n",
    "    'الباحه': 'Al Bahah',\n",
    "    'الحائط': 'Al Hait',\n",
    "    'البرك': 'Al Birk',\n",
    "    'الروضة': 'Ar Rawdah',\n",
    "    'الحجره': 'Al Hijrah',\n",
    "    'بحره': 'Bahrah',\n",
    "    'رابغ': 'Rabigh',\n",
    "    'القويعيه': 'Al Quwaiiyah',\n",
    "    'القصب': 'Al Qasab',\n",
    "    'الشنان': 'Ash Shinan',\n",
    "    'الشملى': 'Al Shumali',\n",
    "    'الدرعيه': 'Ad Diriyah',\n",
    "    'حائل': 'Hail',\n",
    "    'القطيف': 'Qatif',\n",
    "    'حفر الباطن': 'Hafar Al-Batin',\n",
    "    'خليص': 'Khulais',\n",
    "    'بارق': 'Bariq',\n",
    "    'تربه': 'Tarbah',\n",
    "    'وادي الفرع': 'Wadi Al Far\\'',\n",
    "    'جيزان': 'Jizan',\n",
    "    'طريف': 'Turaif',\n",
    "    'الشماسيه': 'Al Shamasiyah',\n",
    "    'نجران': 'Najran',\n",
    "    'الهفوف': 'Al-Hofuf',\n",
    "    'خميس مشيط': 'Khamis Mushait',\n",
    "    'ابو عريش': 'Abu Arish',\n",
    "    'عرعر': 'Arar',\n",
    "    'الجفر': 'Al Jafr',\n",
    "    'عنيزه': 'Unaizah',\n",
    "    'صبياء': 'Sabya',\n",
    "    'الزلفي': 'Az Zulfi',\n",
    "    'البكيريه': 'Al Bukayriyah',\n",
    "    'ابقيق': 'Abqaiq',\n",
    "    'الخبراء': 'Al Khobra',\n",
    "    'المخواة': 'Al Makhwah',\n",
    "    'الرس': 'Ar Rass',\n",
    "    'رياض الخبراء': 'Riyadh Al Khabra',\n",
    "    'سراة عبيده': 'Sarata Ubaidah',\n",
    "    'المذنب': 'Al Mudhnab',\n",
    "    'البدائع': 'Al Badai',\n",
    "    'ابها': 'Abha',\n",
    "    'المزاحميه': 'Al Muzahmiyah',\n",
    "    'صامطه': 'Samtha',\n",
    "    'الخرج': 'Al Kharj',\n",
    "    'بقعاء': 'Buqaa',\n",
    "    'حريملاء': 'Huraymila',\n",
    "    'الجبيل': 'Al Jubail',\n",
    "    'الاحساء': 'Al Ahsa',\n",
    "    'الظهران': 'Ad Dahran',\n",
    "    'القنفذه': 'Al Qunfudhah',\n",
    "    'احد رفيده': 'Ahad Rafidah',\n",
    "    'الخفجي': 'Al Khafji',\n",
    "    'عيون الجوى': 'Uyun Al Jawa',\n",
    "    'ضمد': 'Damad',\n",
    "    'المجمعه': 'Al Majma\\'ah',\n",
    "    'شقراء': 'Shaqra',\n",
    "    'تيماء': 'Tayma',\n",
    "    'بيش': 'Bish',\n",
    "    'القريات': 'Al Qurayyat',\n",
    "    'الغاط': 'Al Ghata',\n",
    "    'محايل عسير': 'Muhayil Asir',\n",
    "    'الخرمه': 'Al Khurma',\n",
    "    'الكهفه': 'Al Kahfah',\n",
    "    'موقق': 'Muqaq',\n",
    "    'الدرب': 'Ad Darb',\n",
    "    'ضريه': 'Dariyah',\n",
    "    'الاسياح': 'As Sayyah',\n",
    "    'ثادق': 'Thadiq',\n",
    "    'املج': 'Umluj',\n",
    "    'الغزاله': 'Al Ghazalah',\n",
    "    'دومة الجندل': 'Dumat Al-Jandal',\n",
    "    'الرين': 'Ar Rayan',\n",
    "    'الارطاوي': 'Al Artawi',\n",
    "    'العيدابي': 'Al Idabi',\n",
    "    'رفحاء': 'Rafha',\n",
    "    'الحريق': 'Al Hariq',\n",
    "    'العيون': 'Al Uyoun',\n",
    "    'عقلة الصقور': 'Uqlat Al Suqur',\n",
    "    'قرية العليا': 'Qariah Al Ulya',\n",
    "    'وادي الدواسر': 'Wadi ad-Dawasir',\n",
    "    'ثار': 'Thar',\n",
    "    'حبونا': 'Habuna',\n",
    "    'قلوه': 'Qilwah',\n",
    "    'العويقيله': 'Al Oyailah',\n",
    "    'المويه': 'Al Muwayh',\n",
    "    'جلاجل': 'Jalajil',\n",
    "    'الجموم': 'Al Jumum',\n",
    "    'ضباء': 'Duba',\n",
    "    'احد المسارحه': 'Ahad Al Masarih',\n",
    "    'باللسمر': 'Balasmar',\n",
    "    'ينبع': 'Yanbu',\n",
    "    'ظهران الجنوب': 'Dhahran Al Janoub',\n",
    "    'فرسان': 'Farsan',\n",
    "    'قبه': 'Qibah',\n",
    "    'المجارده': 'Al Majardah',\n",
    "    'سميراء': 'Sumaira',\n",
    "    'الكامل': 'Al Kamil',\n",
    "    'القواره': 'Al Qawarah',\n",
    "    'النماص': 'An Namas',\n",
    "    'البشائر': 'Al Bashair',\n",
    "    'بلقرن': 'Bilqarn',\n",
    "    'المندق': 'Al Mandaq',\n",
    "    'النبهانيه': 'Al Nabhaniyah',\n",
    "    'لينه': 'Lina',\n",
    "    'حقل': 'Haql',\n",
    "    'السليمي': 'As Sulaymi',\n",
    "    'العرضيه الجنوبيه': 'Al Ardiyah Al Janubiyah',\n",
    "    'بحر ابو سكينه': 'Bahr Abu Sakinah',\n",
    "    'ميسان': 'Maysan',\n",
    "    'رأس تنوره': 'Ras Tanura',\n",
    "    'القرى': 'Al Qurayyah',\n",
    "    'العارضه': 'Al Aridhah',\n",
    "    'الدائر': 'Ad Da\\'ir',\n",
    "    'العرضيه الشماليه': 'Al Ardiyah Ash Shamaliyah',\n",
    "    'الريث': 'Ar Rayth',\n",
    "    'هروب': 'Harub',\n",
    "    'بدر الجنوب': 'Badr Al Janoub',\n",
    "    'بني حسن': 'Bani Hassan',\n",
    "    'الحرث': 'Al Harith',\n",
    "}\n",
    "\n",
    "property_class_map = {\n",
    "    'سكني': 'Residential',\n",
    "    'تجاري': 'Commercial',\n",
    "    'زراعي': 'Agricultural',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34cee72f-3525-4621-a99a-dbd5982612ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Region'] = df['Region'].map(region_mapping).fillna('Unknown')  # Replace nan with 'Unknown'\n",
    "df['City'] = df['City'].map(city_translation).fillna('Unknown')  # Replace nan with 'Unknown'\n",
    "df['Property Classification English'] = df['Property Classification'].map(property_class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d86a6c9-98ee-4df5-b457-a693809c472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region (13 unique):\n",
      "['Unknown' 'Riyadh' 'Najran' 'Asir' 'Al Bahah' 'Eastern Province' 'Medina'\n",
      " 'Jazan' 'Tabuk' 'Al Jawf']\n",
      "--------------------------------------------------\n",
      "City (154 unique):\n",
      "['Taif' 'Riyadh' 'Sharurah' 'Houtat Bani Tamim' 'Ad-Dilam' 'Al-Aflaj'\n",
      " 'Bisha' 'Rijal Almaa' 'Afif' 'Baljurashi']\n",
      "--------------------------------------------------\n",
      "City/Neighborhood (4323 unique):\n",
      "['الطائف/ الجودية ' 'الطائف/ الشهداء الجنوبية ' 'الرياض/الزهراء'\n",
      " 'شروره/ الوديعة ' 'شروره/ النزهة ' 'الرياض/الدريهمية' 'الرياض/بدر'\n",
      " 'الطائف/ وادي جليل ' 'حوطة بني تميم/ الورود ' 'حوطة بني تميم/ نجد ']\n",
      "--------------------------------------------------\n",
      "Transaction ID (65729 unique):\n",
      "[27006520. 27051687. 27038443. 27048618. 27042546. 27045125. 27047908.\n",
      " 27049909. 27050165. 27044359.]\n",
      "--------------------------------------------------\n",
      "Transaction Date (Gregorian) (91 unique):\n",
      "['2025/01/01' '2025/01/02' '2025/01/03' '2025/01/04' '2025/01/05'\n",
      " '2025/01/06' '2025/01/07' '2025/01/08' '2025/01/09' '2025/01/10']\n",
      "--------------------------------------------------\n",
      "Transaction Date (Hijri) (91 unique):\n",
      "['1446/07/01' '1446/07/02' '1446/07/03' '1446/07/04' '1446/07/05'\n",
      " '1446/07/06' '1446/07/07' '1446/07/08' '1446/07/09' '1446/07/10']\n",
      "--------------------------------------------------\n",
      "Property Classification (4 unique):\n",
      "['سكني' 'تجاري' 'زراعي' nan]\n",
      "--------------------------------------------------\n",
      "Property Count (28 unique):\n",
      "[ 1.  3.  2. 11.  8.  6.  4.  7. 12.  5.]\n",
      "--------------------------------------------------\n",
      "Price (12825 unique):\n",
      "['275,000' '523,335' '1,000,000' '1,800,000' '40,000' '150,000' '687,664'\n",
      " '1,300,000' '450,000' '500,000']\n",
      "--------------------------------------------------\n",
      "Area (20224 unique):\n",
      "['760.50' '360.00' '354.42' '348.75' '625.00' '900.00' '156.63' '600.00'\n",
      " '146.40' '935.00']\n",
      "--------------------------------------------------\n",
      "Property Classification English (4 unique):\n",
      "['Residential' 'Commercial' 'Agricultural' nan]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    unique_vals = df[column].unique()\n",
    "    print(f\"{column} ({len(unique_vals)} unique):\")\n",
    "    print(unique_vals[:10])  # show only first 10 values to keep it readable\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c01ec12-a4cb-400f-bcd3-138567f7b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done! Cleaned data saved to 'combined_data_with_price_per_sqm.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Clean and convert Price and Area from strings with commas to numeric\n",
    "df['Price'] = pd.to_numeric(df['Price'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Area'] = pd.to_numeric(df['Area'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "# Rename for clarity (optional but recommended)\n",
    "df.rename(columns={'Area': 'Area_raw'}, inplace=True)\n",
    "\n",
    "# Optional: drop rows with missing or invalid Price/Area\n",
    "df = df.dropna(subset=['Price', 'Area_raw'])\n",
    "\n",
    "# Focus on single-property transactions\n",
    "df = df[df['Property Count'] == 1].copy()\n",
    "\n",
    "# Calculate cleaned area (can skip if not needed)\n",
    "df['Area_clean'] = df['Area_raw']\n",
    "\n",
    "# Calculate price per square meter\n",
    "df['price_per_sqm'] = df['Price'] / df['Area_clean']\n",
    "\n",
    "# Optional: Save updated CSV\n",
    "df.to_csv('combined_data_with_price_per_sqm.csv', index=False)\n",
    "\n",
    "print(\" Done! Cleaned data saved to 'combined_data_with_price_per_sqm.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5f28586-779e-4e66-8417-9525efd3f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region (13 unique):\n",
      "['Unknown' 'Riyadh' 'Najran' 'Asir' 'Al Bahah' 'Eastern Province' 'Medina'\n",
      " 'Jazan' 'Tabuk' 'Al Jawf']\n",
      "--------------------------------------------------\n",
      "City (153 unique):\n",
      "['Taif' 'Riyadh' 'Sharurah' 'Houtat Bani Tamim' 'Ad-Dilam' 'Al-Aflaj'\n",
      " 'Bisha' 'Rijal Almaa' 'Afif' 'Baljurashi']\n",
      "--------------------------------------------------\n",
      "City/Neighborhood (4308 unique):\n",
      "['الطائف/ الجودية ' 'الطائف/ الشهداء الجنوبية ' 'الرياض/الزهراء'\n",
      " 'شروره/ الوديعة ' 'شروره/ النزهة ' 'الرياض/الدريهمية' 'الرياض/بدر'\n",
      " 'الطائف/ وادي جليل ' 'حوطة بني تميم/ الورود ' 'حوطة بني تميم/ نجد ']\n",
      "--------------------------------------------------\n",
      "Transaction ID (65179 unique):\n",
      "[27006520. 27051687. 27038443. 27048618. 27042546. 27045125. 27047908.\n",
      " 27049909. 27050165. 27044359.]\n",
      "--------------------------------------------------\n",
      "Transaction Date (Gregorian) (90 unique):\n",
      "['2025/01/01' '2025/01/02' '2025/01/03' '2025/01/04' '2025/01/05'\n",
      " '2025/01/06' '2025/01/07' '2025/01/08' '2025/01/09' '2025/01/10']\n",
      "--------------------------------------------------\n",
      "Transaction Date (Hijri) (90 unique):\n",
      "['1446/07/01' '1446/07/02' '1446/07/03' '1446/07/04' '1446/07/05'\n",
      " '1446/07/06' '1446/07/07' '1446/07/08' '1446/07/09' '1446/07/10']\n",
      "--------------------------------------------------\n",
      "Property Classification (3 unique):\n",
      "['سكني' 'تجاري' 'زراعي']\n",
      "--------------------------------------------------\n",
      "Property Count (1 unique):\n",
      "[1.]\n",
      "--------------------------------------------------\n",
      "Price (12560 unique):\n",
      "[ 275000.  523335. 1000000. 1800000.   40000.  150000.  687664. 1300000.\n",
      "  450000.  500000.]\n",
      "--------------------------------------------------\n",
      "Area_raw (19921 unique):\n",
      "[760.5  360.   354.42 348.75 625.   900.   156.63 600.   146.4  935.  ]\n",
      "--------------------------------------------------\n",
      "Property Classification English (3 unique):\n",
      "['Residential' 'Commercial' 'Agricultural']\n",
      "--------------------------------------------------\n",
      "Area_clean (19921 unique):\n",
      "[760.5  360.   354.42 348.75 625.   900.   156.63 600.   146.4  935.  ]\n",
      "--------------------------------------------------\n",
      "price_per_sqm (32121 unique):\n",
      "[ 361.60420776 1453.70833333 2821.5112014  5161.29032258   64.\n",
      "  166.66666667 4390.37221477 2166.66666667 3073.7704918   534.75935829]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    unique_vals = df[column].unique()\n",
    "    print(f\"{column} ({len(unique_vals)} unique):\")\n",
    "    print(unique_vals[:10])  # show only first 10 values to keep it readable\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7050928f-407d-4877-a945-eccc5de4bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Performance:\n",
      "MAE: 813.65 SAR/sqm\n",
      "R²: 0.4414\n",
      "✅ Model saved as 'xgb_price_per_sqm_log_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# === 1. Load Data ===\n",
    "# df = pd.read_csv('combined_data.csv', low_memory=False)\n",
    "\n",
    "# === 2. Basic Cleaning & Filtering ===\n",
    "df = df[df['Area_clean'] > 0].copy()\n",
    "df['price_per_sqm'] = df['Price'] / df['Area_clean']\n",
    "\n",
    "df = df[\n",
    "    (df['Area_clean'] >= 50) & (df['Area_clean'] <= 2000) &\n",
    "    (df['Price'] >= 50000) & (df['Price'] <= 5_000_000)\n",
    "].copy()\n",
    "\n",
    "# === 3. Feature Engineering ===\n",
    "df['log_price_per_sqm'] = np.log1p(df['price_per_sqm'])\n",
    "\n",
    "# Group rare cities\n",
    "top_cities = df['City'].value_counts().nlargest(20).index\n",
    "df['City_grouped'] = df['City'].where(df['City'].isin(top_cities), 'Other')\n",
    "\n",
    "# === 4. Select features and target ===\n",
    "features = [\n",
    "    'Area_clean', 'Property Count',\n",
    "    'Region', 'City_grouped',\n",
    "    'Property Classification'\n",
    "]\n",
    "target = 'log_price_per_sqm'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Fill missing categorical data\n",
    "X.loc[:, 'Region'] = X['Region'].fillna('Unknown')\n",
    "X.loc[:, 'City_grouped'] = X['City_grouped'].fillna('Other')\n",
    "X.loc[:, 'Property Classification'] = X['Property Classification'].fillna('Unknown')\n",
    "\n",
    "# === 5. Preprocessing ===\n",
    "categorical_cols = ['Region', 'City_grouped', 'Property Classification']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# === 6. Train/Test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === 7. Model pipeline ===\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === 8. Train ===\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 9. Predict & Evaluate ===\n",
    "y_pred_log = model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)  # inverse of log1p\n",
    "y_test_orig = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "r2 = r2_score(y_test_orig, y_pred)\n",
    "\n",
    "print(f\"✅ Model Performance:\")\n",
    "print(f\"MAE: {mae:.2f} SAR/sqm\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# === 10. Save model ===\n",
    "joblib.dump(model, 'xgb_price_per_sqm_log_model.joblib')\n",
    "print(\"✅ Model saved as 'xgb_price_per_sqm_log_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6490d116-66d7-49e5-945e-9b536f31a0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge R²: 0.5381644591332055\n",
      "RandomForest R²: 0.6181429387472142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Common preprocessing (reuse your preprocessor)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 1. Ridge Regression baseline\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "print(\"Ridge R²:\", ridge_pipeline.score(X_test, y_test))\n",
    "\n",
    "# 2. Random Forest baseline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "print(\"RandomForest R²:\", rf_pipeline.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "831d7e9b-d4be-4ab1-a684-d2b8ec7661a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost R²: 0.6374102418951035\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328\n",
      "[LightGBM] [Info] Number of data points in the train set: 45041, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 7.028155\n",
      "LightGBM R²: 0.6378069822177606\n",
      "CatBoost R²: 0.6169576245048299\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# XGBoost\n",
    "xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "xgb.fit(X_train, y_train)\n",
    "print(\"XGBoost R²:\", xgb.score(X_test, y_test))\n",
    "\n",
    "# LightGBM\n",
    "lgbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=-1,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "lgbm.fit(X_train, y_train)\n",
    "print(\"LightGBM R²:\", lgbm.score(X_test, y_test))\n",
    "\n",
    "# CatBoost (handles categoricals natively if you skip one-hot)\n",
    "cat = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', CatBoostRegressor(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    ))\n",
    "])\n",
    "cat.fit(X_train, y_train)\n",
    "print(\"CatBoost R²:\", cat.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cc9df85-a829-47f9-a227-2b69b251bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor R²: 0.3501470549702723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "nn.fit(X_train, y_train)\n",
    "print(\"MLPRegressor R²:\", nn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2963a47c-7040-42ca-be85-7d69edc2e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best params: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 6, 'regressor__n_estimators': 300}\n",
      "Best CV R²: 0.6368960389695977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [4, 6, 8],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(tree_method='hist', random_state=42))\n",
    "    ]),\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R²:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46973505-b8cf-4844-89fa-316ce83fd7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best params: {'regressor__learning_rate': 0.08903004720036288, 'regressor__max_depth': 6, 'regressor__n_estimators': 369}\n",
      "Best CV R²: 0.6370101061926334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'regressor__n_estimators': randint(100, 500),\n",
    "    'regressor__max_depth': randint(3, 10),\n",
    "    'regressor__learning_rate': uniform(0.01, 0.2)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(tree_method='hist', random_state=42))\n",
    "    ]),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best params:\", random_search.best_params_)\n",
    "print(\"Best CV R²:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0054f348-67f3-4ae1-b6f3-0186042ba8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 23:17:54,377] A new study created in memory with name: no-name-0c53d192-3a9d-4556-b225-ca5602f55ae2\n",
      "C:\\Users\\rhoom\\AppData\\Local\\Temp\\ipykernel_16504\\870085985.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "[I 2025-06-28 23:17:55,604] Trial 0 finished with value: 0.6340351213882706 and parameters: {'n_estimators': 376, 'max_depth': 6, 'learning_rate': 0.035265002287320255}. Best is trial 0 with value: 0.6340351213882706.\n",
      "[I 2025-06-28 23:17:57,475] Trial 1 finished with value: 0.6044507805334671 and parameters: {'n_estimators': 354, 'max_depth': 10, 'learning_rate': 0.18511769278959064}. Best is trial 0 with value: 0.6340351213882706.\n",
      "[I 2025-06-28 23:18:00,257] Trial 2 finished with value: 0.6362031870298978 and parameters: {'n_estimators': 425, 'max_depth': 9, 'learning_rate': 0.016137242985662676}. Best is trial 2 with value: 0.6362031870298978.\n",
      "[I 2025-06-28 23:18:04,008] Trial 3 finished with value: 0.6352648635672623 and parameters: {'n_estimators': 474, 'max_depth': 10, 'learning_rate': 0.011462516283852708}. Best is trial 2 with value: 0.6362031870298978.\n",
      "[I 2025-06-28 23:18:05,583] Trial 4 finished with value: 0.6280539663393386 and parameters: {'n_estimators': 231, 'max_depth': 8, 'learning_rate': 0.01378636746466148}. Best is trial 2 with value: 0.6362031870298978.\n",
      "[I 2025-06-28 23:18:06,935] Trial 5 finished with value: 0.6072942699376852 and parameters: {'n_estimators': 426, 'max_depth': 4, 'learning_rate': 0.01491385157835702}. Best is trial 2 with value: 0.6362031870298978.\n",
      "[I 2025-06-28 23:18:07,634] Trial 6 finished with value: 0.6363500156843496 and parameters: {'n_estimators': 191, 'max_depth': 6, 'learning_rate': 0.11163099471184772}. Best is trial 6 with value: 0.6363500156843496.\n",
      "[I 2025-06-28 23:18:08,642] Trial 7 finished with value: 0.6242645935704765 and parameters: {'n_estimators': 142, 'max_depth': 8, 'learning_rate': 0.019446057128803082}. Best is trial 6 with value: 0.6363500156843496.\n",
      "[I 2025-06-28 23:18:10,479] Trial 8 finished with value: 0.6185654194832241 and parameters: {'n_estimators': 370, 'max_depth': 10, 'learning_rate': 0.09323143661602719}. Best is trial 6 with value: 0.6363500156843496.\n",
      "[I 2025-06-28 23:18:12,467] Trial 9 finished with value: 0.6055693848404752 and parameters: {'n_estimators': 366, 'max_depth': 10, 'learning_rate': 0.17629351550547792}. Best is trial 6 with value: 0.6363500156843496.\n",
      "[I 2025-06-28 23:18:12,949] Trial 10 finished with value: 0.5949359340195083 and parameters: {'n_estimators': 108, 'max_depth': 3, 'learning_rate': 0.07582000476737265}. Best is trial 6 with value: 0.6363500156843496.\n",
      "[I 2025-06-28 23:18:13,963] Trial 11 finished with value: 0.6288066419837882 and parameters: {'n_estimators': 243, 'max_depth': 6, 'learning_rate': 0.032897000714955174}. Best is trial 6 with value: 0.6363500156843496.\n",
      "[I 2025-06-28 23:18:14,943] Trial 12 finished with value: 0.6369547864448065 and parameters: {'n_estimators': 208, 'max_depth': 8, 'learning_rate': 0.06350449214229013}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:15,821] Trial 13 finished with value: 0.6365551282254251 and parameters: {'n_estimators': 201, 'max_depth': 7, 'learning_rate': 0.07598615741167042}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:17,027] Trial 14 finished with value: 0.6368784820671094 and parameters: {'n_estimators': 276, 'max_depth': 7, 'learning_rate': 0.06138948605264287}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:18,267] Trial 15 finished with value: 0.6366108153561777 and parameters: {'n_estimators': 295, 'max_depth': 8, 'learning_rate': 0.05402024673781802}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:19,105] Trial 16 finished with value: 0.6294995708989505 and parameters: {'n_estimators': 287, 'max_depth': 5, 'learning_rate': 0.04653792897136059}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:20,104] Trial 17 finished with value: 0.6273103411924015 and parameters: {'n_estimators': 165, 'max_depth': 7, 'learning_rate': 0.024026435696438503}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:21,082] Trial 18 finished with value: 0.6311028700147211 and parameters: {'n_estimators': 265, 'max_depth': 5, 'learning_rate': 0.05803646648358687}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:22,884] Trial 19 finished with value: 0.6227140411693708 and parameters: {'n_estimators': 324, 'max_depth': 9, 'learning_rate': 0.11963021102047192}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:24,017] Trial 20 finished with value: 0.6334700399600913 and parameters: {'n_estimators': 221, 'max_depth': 7, 'learning_rate': 0.03482476071077508}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:25,421] Trial 21 finished with value: 0.6362672297378272 and parameters: {'n_estimators': 307, 'max_depth': 8, 'learning_rate': 0.055132129861597115}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:26,840] Trial 22 finished with value: 0.6343592891124236 and parameters: {'n_estimators': 264, 'max_depth': 9, 'learning_rate': 0.06211765855172765}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:28,363] Trial 23 finished with value: 0.6366839374447774 and parameters: {'n_estimators': 313, 'max_depth': 8, 'learning_rate': 0.03939788389384702}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:29,291] Trial 24 finished with value: 0.6327640292381447 and parameters: {'n_estimators': 170, 'max_depth': 7, 'learning_rate': 0.03989799507609779}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:31,121] Trial 25 finished with value: 0.6354793283416715 and parameters: {'n_estimators': 327, 'max_depth': 8, 'learning_rate': 0.02341035331110639}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:32,718] Trial 26 finished with value: 0.6361219313603302 and parameters: {'n_estimators': 263, 'max_depth': 9, 'learning_rate': 0.02854827342210887}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:33,506] Trial 27 finished with value: 0.6316265403291927 and parameters: {'n_estimators': 209, 'max_depth': 5, 'learning_rate': 0.07397474171318419}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:35,195] Trial 28 finished with value: 0.6366550427158106 and parameters: {'n_estimators': 404, 'max_depth': 7, 'learning_rate': 0.047807229111411643}. Best is trial 12 with value: 0.6369547864448065.\n",
      "[I 2025-06-28 23:18:36,383] Trial 29 finished with value: 0.6368904867719279 and parameters: {'n_estimators': 338, 'max_depth': 6, 'learning_rate': 0.09883988307509113}. Best is trial 12 with value: 0.6369547864448065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna best params: {'n_estimators': 208, 'max_depth': 8, 'learning_rate': 0.06350449214229013}\n",
      "Optuna best CV R²: 0.6369547864448065\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(**params))\n",
    "    ])\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='r2', n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Optuna best params:\", study.best_params)\n",
    "print(\"Optuna best CV R²:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "436262ee-3951-4957-85ec-623ff1d0039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final XGBoost MAE: 808.17 SAR/sqm\n",
      "Final XGBoost R²: 0.4469\n",
      "✅ Saved 'final_xgb_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Assume df is your cleaned DataFrame, preprocessed as before\n",
    "# Feature engineering, filtering, grouping cities etc. already done\n",
    "\n",
    "features = [\n",
    "    'Area_clean', 'Property Count',\n",
    "    'Region', 'City_grouped',\n",
    "    'Property Classification'\n",
    "]\n",
    "target = 'log_price_per_sqm'\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Fill missing categories\n",
    "for col in ['Region', 'City_grouped', 'Property Classification']:\n",
    "    X[col] = X[col].fillna('Unknown')\n",
    "\n",
    "categorical_cols = ['Region', 'City_grouped', 'Property Classification']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "final_params = {\n",
    "    'n_estimators': 208,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.0635,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "final_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(**final_params))\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = final_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_test_orig = np.expm1(y_test)\n",
    "\n",
    "print(f\"Final XGBoost MAE: {mean_absolute_error(y_test_orig, y_pred):.2f} SAR/sqm\")\n",
    "print(f\"Final XGBoost R²: {r2_score(y_test_orig, y_pred):.4f}\")\n",
    "\n",
    "joblib.dump(final_model, 'final_xgb_model.joblib')\n",
    "print(\"✅ Saved 'final_xgb_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efafeb59-123b-490e-b210-e5f0c73a7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble MAE: 808.78 SAR/sqm\n",
      "Ensemble R²: 0.4470\n",
      "✅ Saved 'ensemble_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define base models with tuned or default params\n",
    "xgb_base = XGBRegressor(\n",
    "    n_estimators=208,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.0635,\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_base = LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cat_base = CatBoostRegressor(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    random_seed=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Stacking ensemble\n",
    "stack = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_base),\n",
    "        ('lgbm', lgbm_base),\n",
    "        ('cat', cat_base)\n",
    "    ],\n",
    "    final_estimator=RidgeCV(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ensemble_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('stacker', stack)\n",
    "])\n",
    "\n",
    "ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = ensemble_pipeline.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "print(f\"Ensemble MAE: {mean_absolute_error(y_test_orig, y_pred):.2f} SAR/sqm\")\n",
    "print(f\"Ensemble R²: {r2_score(y_test_orig, y_pred):.4f}\")\n",
    "\n",
    "joblib.dump(ensemble_pipeline, 'ensemble_model.joblib')\n",
    "print(\"✅ Saved 'ensemble_model.joblib'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
